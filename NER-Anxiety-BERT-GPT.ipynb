{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5edd9ec9-334f-4027-b8be-11870692533c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BERT&GPT-NER-Token Classification-Anxiety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd91a6-02d6-441a-a4dd-b01f33017a8f",
   "metadata": {},
   "source": [
    "## Automated BIO Tagging for Anxiety Symptoms Using spaCy\n",
    "\n",
    "This Python script utilizes the `spaCy` library to perform Named Entity Recognition (NER) for identifying mentions of anxiety in a corpus of text. The goal is to tag these mentions using the BIO (Begin, Inside, Outside) tagging format.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "- **spaCy Model Loading**: Loads the English language model from spaCy.\n",
    "- **Data Loading**: Reads a CSV file containing the corpus into a pandas DataFrame.\n",
    "- **Phrase Matching**: Utilizes spaCy's `PhraseMatcher` to find sequences of words that match terms indicative of anxiety.\n",
    "- **BIO Tagging Function**: A custom function `bio_tagging_spacy` that applies BIO tagging to the identified terms within the text.\n",
    "- **DataFrame Creation**: Constructs a new DataFrame from the tagged data, which pairs each token with its corresponding BIO tag.\n",
    "- **Output**: Saves the tagged data to a new CSV file for further analysis or training machine learning models.\n",
    "\n",
    "### Output\n",
    "\n",
    "The script outputs a CSV file named `df_tokens_Anxiety.csv`. This file contains three columns: `goldID`, `token`, and `tag`, where each token from the corpus is annotated with its respective BIO tag indicating whether it is a part of an anxiety mention.\n",
    "\n",
    "By automating the tagging process, this script facilitates more efficient preparation of text data for complex NLP tasks such as training models for sentiment analysis or more detailed psychological state assessments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435b2363-fd0a-43fa-8cc6-613f1192e28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nzeinali/.local/lib/python3.8/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.6.0) was trained with spaCy v3.6.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   goldID        token tag\n",
      "0     356  authorizing   O\n",
      "1     356     provider   O\n",
      "2     356       younke   O\n",
      "3     356       denise   O\n",
      "4     356            l   O\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('new_corpus_14_symptoms_counted.csv')\n",
    "df_Anxiety = df[[\"Unnamed: 0\", \"goldID\", \"text\", \"Anxiety\"]]\n",
    "df_dic = pd.read_csv('Anxiety_dic.csv', encoding='latin1')\n",
    "\n",
    "# Existing terms from your dictionary\n",
    "Anxiety_terms = df_dic['Anxiety'].tolist()\n",
    "\n",
    "patterns = [nlp.make_doc(term.lower()) for term in Anxiety_terms]\n",
    "\n",
    "# Create the PhraseMatcher object\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "matcher.add(\"AnxietyPattern\", None, *patterns)\n",
    "\n",
    "# Function to apply BIO tagging using spaCy's PhraseMatcher\n",
    "def bio_tagging_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    tokens = [token.text for token in doc]\n",
    "    labels = ['O'] * len(tokens)  # Default labels\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        labels[start] = 'B-Anxiety'  # Begin tag, you could use 'B-Vomit' if specific tagging is required\n",
    "        for i in range(start + 1, end):\n",
    "            labels[i] = 'I-Anxiety'  # Inside tag, similarly 'I-Vomit'\n",
    "\n",
    "    return list(zip(tokens, labels))\n",
    "\n",
    "# Apply BIO tagging and create a new DataFrame\n",
    "token_data = []\n",
    "for _, row in df_Anxiety.iterrows():\n",
    "    result = bio_tagging_spacy(row['text'])\n",
    "    for token, tag in result:\n",
    "        token_data.append({'goldID': row['goldID'], 'token': token, 'tag': tag})\n",
    "\n",
    "df_tokens = pd.DataFrame(token_data)\n",
    "\n",
    "# Save and display the results\n",
    "df_tokens.to_csv('df_tokens_Anxiety.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b911330-1f71-4d41-a845-b231a01409b4",
   "metadata": {},
   "source": [
    "## BERT-Based Named Entity Recognition for Anxiety Symptoms\n",
    "\n",
    "This section of the Python script is designed to set up a BERT-based named entity recognition (NER) system for identifying anxiety symptoms in text. It uses libraries such as PyTorch, Hugging Face's transformers, and spaCy to perform text processing and entity tagging.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The code performs the following functions:\n",
    "\n",
    "1. **Imports and Logger Setup**: The necessary libraries are imported, and logging is configured for better debugging and tracking.\n",
    "2. **Model Configuration**: Defines model names and configurations for different versions of BERT models tailored for clinical text (e.g., Bio-ClinicalBERT).\n",
    "3. **Label Mapping**: Sets up the mapping between labels and their corresponding IDs for classification.\n",
    "4. **Data Preparation**: Functions to load and preprocess the data are defined, ensuring the data fits the model requirements.\n",
    "5. **Dataset and DataLoader**: Implements custom PyTorch dataset and dataloader to handle the tokenization and encoding of text data.\n",
    "6. **Model Training and Evaluation**: Functions to train and evaluate the model using the provided datasets, calculating metrics such as precision, recall, and F1-score.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "- **Config Class**: Holds configuration constants like batch size, learning rate, and device (CPU/GPU).\n",
    "- **SentenceDataset Class**: Custom dataset class for handling sentence tokenization and encoding.\n",
    "- **Training and Evaluation Functions**: Include detailed logging and performance metrics tracking to monitor the training process and evaluate the model's effectiveness.\n",
    "\n",
    "### Output\n",
    "\n",
    "The script outputs training and test loss, along with precision, recall, and F1 scores for the test dataset. Additionally, it saves these metrics to a CSV file in a results directory for further analysis.\n",
    "\n",
    "### Execution\n",
    "\n",
    "The main function orchestrates the loading of data, model training, and evaluation. It also ensures the results are saved and properly logged. This setup allows for a robust evaluation of different BERT models on the specific task of anxiety symptom recognition in clinical texts.\n",
    "\n",
    "By structuring and training models in this way, researchers and practitioners can develop more effective tools for automatic symptom detection, aiding in faster and more accurate clinical assessments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be94a469-23ee-4d9a-99ab-629ae20da50d",
   "metadata": {},
   "source": [
    "## Step By Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c79a68-11c0-45ce-b239-bb235079292a",
   "metadata": {},
   "source": [
    "## Setup for BERT-Based NER Model\n",
    "\n",
    "This section of the Python script sets up the foundation for a Named Entity Recognition (NER) system specifically tailored to identify anxiety symptoms in text. Utilizing PyTorch and the transformers library, it configures logging, model parameters, and data handling mechanisms necessary for the NER task.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "The script imports necessary libraries such as `pandas` for data manipulation, `torch` and its utilities for deep learning operations, and `transformers` for accessing pre-trained BERT models and tokenizers. Additionally, metrics from `seqeval` are used to evaluate model performance.\n",
    "\n",
    "### Configuration and Logging\n",
    "\n",
    "- **Logging**: Configured to provide timestamped logs at the INFO level, helping in debugging and tracking model training and evaluation.\n",
    "- **Model Names**: A dictionary mapping model identifiers to their descriptive names, facilitating easy switches between different BERT models.\n",
    "- **Label Mapping**: Defines a mapping of NER labels to numerical IDs, essential for model training and inference.\n",
    "\n",
    "### System Configuration Class\n",
    "\n",
    "A `Config` class contains all relevant system and model settings:\n",
    "- **Maximum Sequence Length**: Defines the cutoff length for tokenization.\n",
    "- **Batch Sizes**: Specifies different batch sizes for training and validation phases.\n",
    "- **Training Parameters**: Includes settings such as the number of epochs, learning rate, and gradient clipping norm.\n",
    "- **Device Setup**: Automatically assigns the model to run on GPU if available, otherwise uses CPU.\n",
    "- **Data and Results Handling**: Designates paths for the input data and directory for saving results.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This setup ensures that all components of the NER system are configured properly before training begins. It enables the model to efficiently process text data, learn from it, and store the results for further analysis or deployment in clinical settings.\n",
    "\n",
    "By centralizing configuration settings and logging mechanisms, the script maintains high modularity and ease of maintenance, making adjustments and upgrades straightforward as new data or models become available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805fc0b7-b6fc-4737-8db3-953f3dfb924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from torch import cuda\n",
    "import logging\n",
    "import os\n",
    "\n",
    "#Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define model names and their friendly names\n",
    "model_names = {\n",
    "    \"bert-base-uncased\": \"BERT\",\n",
    "    \"emilyalsentzer/Bio_ClinicalBERT\": \"Bio-ClinicalBERT\",\n",
    "    \"New_Bio-Clinical_BERT_finetuned\": \"Symptom_BERT\"\n",
    "}\n",
    "\n",
    "# Define label mapping\n",
    "label2id = {'O': 0, 'B-Anxiety': 1, 'I-Anxiety': 2}  # Adjust as necessary\n",
    "id2label = {v: k for k, v in label2id.items()}  # Reverse mapping from ID to label\n",
    "\n",
    "class Config:\n",
    "    MAX_LEN = 512\n",
    "    TRAIN_BATCH_SIZE = 4\n",
    "    VALID_BATCH_SIZE = 2\n",
    "    EPOCHS = 5\n",
    "    LEARNING_RATE = 3e-05\n",
    "    MAX_GRAD_NORM = 10\n",
    "    TRAIN_SIZE = 0.8\n",
    "    DEVICE = 'cuda' if cuda.is_available() else 'cpu'\n",
    "    DATA_FILE = \"df_tokens_Anxiety.csv\"\n",
    "    RESULTS_DIR = './results_Anxiety_NER_BERT&GPT'  # Directory to save results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad479dbe-abb3-4b45-a7bd-0d5d54aa287e",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing Function\n",
    "\n",
    "The `load_data` function is designed to handle the initial loading and preprocessing of the dataset necessary for training the NER model. This function is crucial for ensuring the data is in the correct format for tokenization and sequence labeling.\n",
    "\n",
    "### Function Details\n",
    "\n",
    "- **Input**: Accepts a file path to the CSV file containing the annotated tokens.\n",
    "- **Processing Steps**:\n",
    "  1. **Reading the Data**: The data is loaded using `pandas` with `unicode_escape` encoding to handle any special characters.\n",
    "  2. **Handling Missing Values**: Uses forward fill (`ffill`) to handle any missing values in the data, ensuring no gaps in token sequences.\n",
    "  3. **Sentence Reconstruction**: Aggregates tokens by their corresponding `goldID` to reconstruct the original sentences.\n",
    "  4. **Label Aggregation**: Similarly, aggregates the BIO tags to form the corresponding label sequences for each sentence.\n",
    "  5. **Deduplication**: Removes duplicate entries to ensure the uniqueness of each training example.\n",
    "  \n",
    "- **Output**: Returns a DataFrame with two columns: `sentence` and `word_labels`, which contain the reconstructed sentences and their respective label sequences.\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "- The function includes robust error handling to log any issues encountered during the data loading process. If an error occurs, it raises an exception to halt further execution, ensuring that no faulty data compromises the training process.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This preprocessing step is vital for converting raw tokenized data into a structured format that the BERT tokenizer can effectively process. By preparing the data meticulously, this function helps in maintaining the integrity and quality of the training data, leading to more reliable NER model performance.\n",
    "\n",
    "This structured approach not only facilitates error tracking and debugging but also enhances the model's ability to learn from accurately labeled data, improving its efficacy in real-world applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a658df8-55c5-4f49-9718-5123ccb52d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, encoding='unicode_escape')\n",
    "        data = data.fillna(method='ffill')\n",
    "        data['sentence'] = data.groupby(['goldID'])['token'].transform(lambda x: ' '.join(x))\n",
    "        data['word_labels'] = data.groupby(['goldID'])['tag'].transform(lambda x: ','.join(x))\n",
    "        data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load data: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a718f-efc9-4bc9-b5f0-aba9412e1c5e",
   "metadata": {},
   "source": [
    "## Dataset Preparation Function\n",
    "\n",
    "The `prepare_datasets` function is pivotal for splitting the preprocessed data into training and testing datasets, essential for training and evaluating the NER model's performance.\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "- **Input**: Accepts a `pandas` DataFrame that contains the entire dataset.\n",
    "- **Functionality**:\n",
    "  1. **Sampling Training Data**: Randomly samples a specified fraction (`Config.TRAIN_SIZE`) of the data for training. The sampling is reproducible due to the fixed `random_state`.\n",
    "  2. **Creating Test Data**: Identifies and separates the remainder of the data to form the test dataset.\n",
    "  \n",
    "- **Output**: Returns two DataFrames:\n",
    "  - `train_data`: The dataset intended for training the model.\n",
    "  - `test_data`: The dataset reserved for validating the model's performance.\n",
    "\n",
    "### Configuration Dependency\n",
    "\n",
    "- The function leverages the `TRAIN_SIZE` parameter from the `Config` class to determine the proportion of data used for training. This parameter can be adjusted depending on the dataset size or specific training requirements.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Splitting the data into training and testing sets is crucial for machine learning models to:\n",
    "- **Learn Patterns** (training phase): The model learns to recognize patterns and associations within the labeled training data.\n",
    "- **Evaluate Accuracy** (testing phase): The model's predictions are compared against actual labels in the test dataset to evaluate its accuracy and generalizability.\n",
    "\n",
    "By properly preparing these datasets, this function ensures that the NER model is trained in a controlled environment and evaluated objectively, facilitating the development of a robust and effective entity recognition system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d7206-81c8-41b3-8ff1-854f2f148761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "def prepare_datasets(data):\n",
    "    train_data = data.sample(frac=Config.TRAIN_SIZE, random_state=200)\n",
    "    test_data = data.drop(train_data.index).reset_index(drop=True)\n",
    "    return train_data.reset_index(drop=True), test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a804b9-1bde-4398-84f8-147925492fc3",
   "metadata": {},
   "source": [
    "## Custom Dataset Class for NER Model\n",
    "\n",
    "The `SentenceDataset` class is a custom implementation extending PyTorch's `Dataset` class. It is specifically tailored to process text data for Named Entity Recognition (NER) tasks using BERT models.\n",
    "\n",
    "### Class Structure\n",
    "\n",
    "- **Initialization (`__init__`)**: Configures the dataset with the necessary components.\n",
    "  - `dataframe`: A DataFrame containing the sentences and their corresponding labels.\n",
    "  - `tokenizer`: A BERT tokenizer to convert text into tokens that the model can understand.\n",
    "  - `max_len`: The maximum length of the sequences to be processed, ensuring consistency in input size.\n",
    "\n",
    "- **Length Determination (`__len__`)**: Returns the number of items in the dataset, allowing PyTorch's `DataLoader` to plan batching and shuffling operations.\n",
    "\n",
    "- **Item Access (`__getitem__`)**: Retrieves a single processed item from the dataset by index.\n",
    "  - **Sentence and Label Extraction**: Extracts the sentence and its labels from the DataFrame.\n",
    "  - **Tokenization and Label Preservation**: Tokenizes the sentence and adjusts the labels to align with the tokenized output.\n",
    "  - **Encoding**: Converts tokens and labels into the format required by BERT, including adding special tokens, converting to IDs, and applying padding.\n",
    "\n",
    "### Detailed Functionality\n",
    "\n",
    "- **Tokenization (`tokenize_and_preserve_labels`)**: Splits the sentence into tokens/subwords and duplicates the labels accordingly to match the length of the generated tokens, preserving the label alignment.\n",
    "  \n",
    "- **Encoding (`encode_plus`)**: Prepares the final tokenized input for the model. This includes:\n",
    "  - Adding special tokens (`[CLS]`, `[SEP]`) necessary for BERT.\n",
    "  - Creating attention masks to differentiate real tokens from padding.\n",
    "  - Converting tokens to their corresponding IDs.\n",
    "  - Adjusting label IDs for compatibility with loss functions, using a default ID for padding tokens.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This class is crucial for transforming raw text data into a structured format that can be directly fed into a BERT model for training or inference. By ensuring that each text entry is tokenized and encoded correctly, the `SentenceDataset` class supports effective model training and contributes to higher model performance on NER tasks.\n",
    "\n",
    "The design of this class also facilitates easy integration and usage within PyTorch training frameworks, making it a versatile component for various NLP tasks involving BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca28a7-e4fc-411a-8e7c-38f544b9e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Dataset class\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.iloc[index]['sentence']\n",
    "        word_labels = self.data.iloc[index]['word_labels']\n",
    "        tokenized_sentence, labels = self.tokenize_and_preserve_labels(sentence, word_labels)\n",
    "        return self.encode_plus(tokenized_sentence, labels)\n",
    "\n",
    "    def tokenize_and_preserve_labels(self, sentence, text_labels):\n",
    "        tokenized_sentence, labels = [], []\n",
    "        for word, label in zip(sentence.split(), text_labels.split(',')):\n",
    "            subwords = self.tokenizer.tokenize(word)\n",
    "            tokenized_sentence.extend(subwords)\n",
    "            labels.extend([label] * len(subwords))\n",
    "        return tokenized_sentence, labels\n",
    "   \n",
    "    def encode_plus(self, tokenized_sentence, labels):\n",
    "        tokenized_sentence = ['[CLS]'] + tokenized_sentence[:self.max_len-2] + ['[SEP]']\n",
    "        labels = ['O'] + labels[:self.max_len-2] + ['O']\n",
    "        attention_mask = [1] * len(tokenized_sentence) + [0] * (self.max_len - len(tokenized_sentence))\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "        label_ids = [label2id.get(label, -100) for label in labels]\n",
    "        padding_length = self.max_len - len(input_ids)\n",
    "        input_ids += [self.tokenizer.pad_token_id] * padding_length\n",
    "        label_ids += [-100] * padding_length\n",
    "        return {'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'labels': torch.tensor(label_ids, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39784ca-1e4c-431e-bcb6-75446805012e",
   "metadata": {},
   "source": [
    "## Training Function for NER Model\n",
    "\n",
    "The `train` function is designed to handle the training process of a BERT-based NER model using PyTorch. It encapsulates the steps necessary to iterate over batches of data, compute the loss, and update the model's weights.\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "- **Inputs**:\n",
    "  - `model`: The NER model to be trained, pre-initialized with the necessary architecture and weights.\n",
    "  - `loader`: A DataLoader that provides batches of the dataset.\n",
    "  - `optimizer`: The optimization algorithm used to update the weights based on the computed gradients.\n",
    "\n",
    "### Core Operations\n",
    "\n",
    "1. **Mode Setting**: Sets the model to training mode, enabling features like dropout layers that are essential during the training phase but not during evaluation.\n",
    "2. **Loss Initialization**: Initializes the total loss to zero. This will accumulate the loss over all batches in the dataset.\n",
    "3. **Batch Processing**:\n",
    "   - For each batch, the function extracts `input_ids`, `attention_mask`, and `labels`, and moves them to the appropriate device (CPU or GPU).\n",
    "   - Clears old gradients from the last step (if existing).\n",
    "4. **Forward Pass**:\n",
    "   - The model performs a forward pass to compute the logits from the input data.\n",
    "   - Computes the loss between the logits and the ground-truth labels.\n",
    "5. **Backward Pass**:\n",
    "   - Backpropagation is used to compute the gradient of the loss function with respect to the model parameters.\n",
    "   - Gradient clipping is applied to prevent exploding gradients, which can destabilize the training process.\n",
    "6. **Optimization Step**:\n",
    "   - Updates the model parameters based on the gradients computed during backpropagation.\n",
    "\n",
    "### Output\n",
    "\n",
    "- Returns the average loss computed over all batches. This metric is essential for monitoring the training process and diagnosing issues with model convergence.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This training function is critical for optimizing the NER model, allowing it to learn from the training data effectively. By iteratively adjusting the model's weights, the function seeks to minimize the loss, thus enhancing the model's ability to accurately predict entity labels.\n",
    "\n",
    "Structured and well-documented, this function is vital for achieving high performance in NER tasks, facilitating robust model training through systematic iteration and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9804716-7e15-4833-b5f8-7f75214542c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        inputs, masks, labels = batch['input_ids'].to(Config.DEVICE), batch['attention_mask'].to(Config.DEVICE), batch['labels'].to(Config.DEVICE)\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids=inputs, attention_mask=masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), Config.MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd084759-bacb-48f4-bfb7-01a69c870364",
   "metadata": {},
   "source": [
    "## Evaluation Function for NER Model\n",
    "\n",
    "The `evaluate` function is crucial for assessing the performance of a trained NER model using a validation or test dataset. It measures the model's ability to predict entity labels accurately against the known labels.\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "- **Inputs**:\n",
    "  - `model`: The NER model to be evaluated.\n",
    "  - `loader`: A DataLoader that supplies batches of the dataset for evaluation.\n",
    "\n",
    "### Core Operations\n",
    "\n",
    "1. **Mode Setting**: Puts the model in evaluation mode, which deactivates training-specific features like dropout to stabilize the model’s predictions.\n",
    "2. **Loss and Prediction Tracking**: Initializes variables to track the total loss and predictions across all batches.\n",
    "3. **Batch Processing**:\n",
    "   - Processes each batch in a no-gradient context to prevent updates to model parameters and reduce memory consumption.\n",
    "   - For each batch, extracts inputs and their corresponding labels and transfers them to the appropriate device.\n",
    "4. **Model Prediction**:\n",
    "   - Conducts a forward pass to generate logits from the input data.\n",
    "   - Computes the loss to gauge the discrepancy between predicted and actual labels.\n",
    "   - Accumulates the total loss for later averaging.\n",
    "5. **Label Prediction Extraction**:\n",
    "   - Converts logits to actual label predictions using the `argmax` function, which selects the most likely label for each token.\n",
    "   - Extracts labels from both the predictions and the ground truth where the attention mask indicates actual token presence (ignoring padded areas).\n",
    "6. **Metrics Computation**:\n",
    "   - Calculates precision, recall, and F1-score to evaluate the model's performance on identifying correct labels accurately.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "- Returns the average loss over all batches and the precision, recall, and F1 scores, providing a comprehensive view of the model's performance.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Evaluating the model using these metrics is essential for understanding its effectiveness in real-world scenarios and identifying areas for improvement. This function provides a systematic approach to quantify the model's accuracy and generalizability, ensuring robust performance in NER tasks.\n",
    "\n",
    "Structured to facilitate thorough performance assessment, this function is key to validating the model’s capability and ensuring it meets the expected standards of accuracy and efficiency in entity recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c339a6-2212-4228-9517-5e1ef741d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs, masks, targets = batch['input_ids'].to(Config.DEVICE), batch['attention_mask'].to(Config.DEVICE), batch['labels'].to(Config.DEVICE)\n",
    "            outputs = model(input_ids=inputs, attention_mask=masks, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions_batch = torch.argmax(logits, axis=2)\n",
    "            for i, mask in enumerate(masks):\n",
    "                temp_1 = []\n",
    "                temp_2 = []\n",
    "                for j, m in enumerate(mask):\n",
    "                    if m and targets[i, j] != torch.tensor(-100):\n",
    "                        temp_1.append(id2label[targets[i, j].item()])\n",
    "                        temp_2.append(id2label[predictions_batch[i, j].item()])\n",
    "                labels.append(temp_1)\n",
    "                predictions.append(temp_2)\n",
    "\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    return total_loss / len(loader), precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb329574-049d-48ca-a502-12662216d253",
   "metadata": {},
   "source": [
    "## Main Execution Flow for Training and Evaluating NER Models\n",
    "\n",
    "The `main` function orchestrates the overall process of training and evaluating named entity recognition (NER) models. It sets up the environment, initializes data, configures models, and conducts training and evaluation cycles.\n",
    "\n",
    "### Process Overview\n",
    "\n",
    "1. **Data Loading and Preparation**:\n",
    "   - Loads the dataset from a specified file using the `load_data` function.\n",
    "   - Prepares the dataset into training and testing sets using the `prepare_datasets` function.\n",
    "\n",
    "2. **Environment Setup**:\n",
    "   - Checks and creates a results directory if it doesn't exist, ensuring a place to save the output results.\n",
    "\n",
    "3. **Model Training Setup**:\n",
    "   - Iterates through predefined model configurations.\n",
    "   - For each configuration:\n",
    "     - Initializes the tokenizer and model with the specified BERT variant.\n",
    "     - Transfers the model to the appropriate compute device (CPU or GPU).\n",
    "     - Prepares data loaders for both training and testing datasets with specified batch sizes and shuffle configurations.\n",
    "     - Initializes the optimizer with a defined learning rate.\n",
    "\n",
    "4. **Training Loop**:\n",
    "   - Conducts multiple epochs of training:\n",
    "     - Each epoch involves running the `train` function to process the training data through the model and update the weights.\n",
    "     - Logs the loss at the end of each epoch for monitoring.\n",
    "\n",
    "5. **Final Epoch Evaluation**:\n",
    "   - In the last epoch, evaluates the model on the test dataset using the `evaluate` function.\n",
    "   - Extracts and logs performance metrics such as test loss, precision, recall, and F1 score.\n",
    "\n",
    "6. **Results Handling**:\n",
    "   - Prints the collected test metrics for quick reference.\n",
    "   - Saves the metrics to a CSV file in the predefined results directory.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The main function is designed to provide a comprehensive and systematic approach to training and evaluating deep learning models for NER tasks. It integrates various components and functions, ensuring a streamlined workflow from data preparation to performance evaluation.\n",
    "\n",
    "This approach not only facilitates efficient training cycles but also ensures that the models are rigorously tested and their performance metrics are accurately captured and reported, crucial for iterative model development and refinement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0788dc8b-69fd-4012-8d57-30e357841529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 11:15:30,890 - INFO - Training and evaluating model: BERT\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-05-17 11:15:31,599 - INFO - Training and evaluating model: Bio-ClinicalBERT\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-05-17 11:16:21,227 - INFO - Epoch 1, Train Loss: 0.03271314332635294\n",
      "2024-05-17 11:17:10,789 - INFO - Epoch 2, Train Loss: 0.008752769470041574\n",
      "2024-05-17 11:18:00,889 - INFO - Epoch 3, Train Loss: 0.0026288037556210036\n",
      "2024-05-17 11:18:51,482 - INFO - Epoch 4, Train Loss: 0.0007800225708241098\n",
      "2024-05-17 11:19:42,413 - INFO - Epoch 5, Train Loss: 0.00038744608849134067\n",
      "2024-05-17 11:19:47,401 - INFO - Precision: 0.9210526315789473, Recall: 0.813953488372093, F1-Score: 0.8641975308641974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'Test Loss': 0.00427703428660472, 'Precision': 0.9210526315789473, 'Recall': 0.813953488372093, 'F1 Score': 0.8641975308641974}\n",
      "*********************************************************************************************\n",
      "Saved test metrics to ./results_Anxiety_NER_BERT&GPT/Bio-ClinicalBERT_test_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "def main():\n",
    "    data = load_data(Config.DATA_FILE)\n",
    "    train_data, test_data = prepare_datasets(data)\n",
    "    \n",
    "    if not os.path.exists(Config.RESULTS_DIR):\n",
    "        os.makedirs(Config.RESULTS_DIR)\n",
    "        \n",
    "    for model_key in model_names:\n",
    "        logging.info(f\"Training and evaluating model: {model_names[model_key]}\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_key)\n",
    "        model = BertForTokenClassification.from_pretrained(model_key, num_labels=len(id2label))\n",
    "        model.to(Config.DEVICE)\n",
    "        train_loader = DataLoader(SentenceDataset(train_data, tokenizer, Config.MAX_LEN), batch_size=Config.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(SentenceDataset(test_data, tokenizer, Config.MAX_LEN), batch_size=Config.VALID_BATCH_SIZE, shuffle=False)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        train_loss = train(model, train_loader, optimizer)\n",
    "        logging.info(f'Epoch {epoch+1}, Train Loss: {train_loss}')\n",
    "        if epoch == Config.EPOCHS - 1:  # Only evaluate and save in the last epoch\n",
    "            test_loss, precision, recall, f1 = evaluate(model, test_loader)\n",
    "            test_metrics = {'Test Loss': test_loss, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
    "\n",
    "\n",
    "         # Now outside the loop, only for the final epoch\n",
    "    print(f\"Test metrics: {test_metrics}\")\n",
    "    print(\"*********************************************************************************************\")\n",
    "        \n",
    "    # Save metrics\n",
    "    metrics_filename = os.path.join(Config.RESULTS_DIR, f\"{model_names[model_key]}_test_metrics.csv\")\n",
    "    pd.DataFrame([test_metrics]).to_csv(metrics_filename, index=False)\n",
    "    print(f\"Saved test metrics to {metrics_filename}\")\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd7ab68-07df-419c-ad67-0defebdd9893",
   "metadata": {},
   "source": [
    "## GPT-NER-Token Classification-Anxiety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0bf55-a1ee-4abb-b379-4058d431dffe",
   "metadata": {},
   "source": [
    "## Python Setup for GPT-2 Based Token Classification\n",
    "\n",
    "This script sets the foundation for a token classification task using variants of the GPT-2 model, tailored for Named Entity Recognition (NER). It configures the necessary environment, including data handling, model selection, and logging.\n",
    "\n",
    "### Imports and Dependencies\n",
    "\n",
    "- **Libraries**: Utilizes `pandas` for data handling, `torch` for constructing deep learning models, and `transformers` for accessing pretrained GPT-2 models and tokenizers.\n",
    "- **Metrics**: Employs `seqeval` metrics to evaluate the classification performance, including precision, recall, and F1 score.\n",
    "\n",
    "### Configuration and Logging\n",
    "\n",
    "- **Logging Setup**: Configures basic logging to track and log runtime events and model performance metrics. (Note: This line is commented out in the snippet provided.)\n",
    "- **Model Selection**: Specifies a dictionary of GPT-2 model variants. Each key is a model identifier with its respective name. (Note: Most entries are commented out, with only the base 'gpt2' model active for use.)\n",
    "\n",
    "### Label Mapping\n",
    "\n",
    "- **Label Definitions**: Maps entity labels to numeric IDs, essential for model training and prediction. This setup uses an example mapping for anxiety-related labels (`B-Anxiety`, `I-Anxiety`).\n",
    "- **Reverse Mapping**: Provides a mechanism to convert numeric IDs back to their corresponding textual labels.\n",
    "\n",
    "### System Configuration Class\n",
    "\n",
    "Defines several crucial constants and parameters for the model training and evaluation:\n",
    "- **`MAX_LEN`**: Maximum sequence length to which inputs will be padded or truncated.\n",
    "- **`TRAIN_BATCH_SIZE`** and **`VALID_BATCH_SIZE`**: Specifies the sizes of data batches for training and validation.\n",
    "- **`EPOCHS`** and **`LEARNING_RATE`**: Defines the number of training cycles and the step size for updating model weights.\n",
    "- **`MAX_GRAD_NORM`**: Sets a limit for gradient norm clipping to prevent exploding gradients during backpropagation.\n",
    "- **`TRAIN_SIZE`**: Fraction of data used for training, with the remainder used for validation.\n",
    "- **`DEVICE`**: Automatically selects CUDA if available (for GPU acceleration) or defaults to CPU.\n",
    "- **`DATA_FILE`** and **`RESULTS_DIR`**: Paths for loading the dataset and saving results, respectively.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This setup script is designed to initialize the environment and configurations for training and evaluating a GPT-2 based NER model. It ensures all components are in place for effective model operation, from data preprocessing to model training and performance evaluation. By organizing these settings at the start, the script facilitates a smooth and efficient workflow for complex NER tasks involving deep learning.\n",
    "\n",
    "### Usage\n",
    "\n",
    "To run NER tasks with the configured settings, further scripts would utilize these configurations to prepare data, train models, and evaluate results. This setup ensures that all components are optimized for the specific demands of token classification using advanced language models like GPT-2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f3931-4217-44d8-9248-e6a84c549c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2ForTokenClassification\n",
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from torch import cuda\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "model_names = {\n",
    "    'gpt2': 'gpt2',\n",
    "    #'biogpt': 'microsoft/BioGPT',\n",
    "    #'bioMedLM': 'stanford-crfm/BioMedLM',\n",
    "    'symptom-GPT': './symptom-BioGPT-1 Million',\n",
    "    #'symptom-GPT-Neo': 'EleutherAI/gpt-neo-1.3B'\n",
    "}\n",
    "# Define label mapping\n",
    "label2id = {'O': 0, 'B-Anxiety': 1, 'I-Anxiety': 2}  # Example, adjust according to your actual labels\n",
    "id2label = {v: k for k, v in label2id.items()}  # Reverse mapping from ID to label\n",
    "\n",
    "# Constants and Configurations\n",
    "class Config:\n",
    "    MAX_LEN = 512\n",
    "    TRAIN_BATCH_SIZE = 4\n",
    "    VALID_BATCH_SIZE = 2\n",
    "    EPOCHS = 5\n",
    "    LEARNING_RATE = 3e-05\n",
    "    MAX_GRAD_NORM = 10\n",
    "    TRAIN_SIZE = 0.8\n",
    "    DEVICE = 'cuda' if cuda.is_available() else 'cpu'\n",
    "    DATA_FILE = \"df_tokens_Anxiety.csv\"\n",
    "    RESULTS_DIR = './results_Anxiety_NER_BERT&GPT'  # Directory to save results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f2a68-ef54-4b82-998a-337a5f216153",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing Function\n",
    "\n",
    "The `load_data` function is crucial for the initial steps of loading and preprocessing the dataset required for training the token classification model. This function ensures the data is in an appropriate format for further processing and analysis.\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "- **Input**: Accepts the file path to a CSV file containing the dataset.\n",
    "- **Exception Handling**: Implements robust error handling to manage and log issues that occur during data loading.\n",
    "\n",
    "### Core Operations\n",
    "\n",
    "1. **CSV Data Loading**: \n",
    "   - Loads the dataset from the specified CSV file using `pandas`, with `unicode_escape` encoding to correctly handle special characters.\n",
    "   \n",
    "2. **Missing Value Handling**: \n",
    "   - Applies forward fill (`ffill`) to fill any missing values in the data. This method ensures that all data points have complete information by propagating the last valid observation forward.\n",
    "\n",
    "3. **Sentence and Label Aggregation**: \n",
    "   - Groups tokens by `goldID` and concatenates them to reconstruct full sentences, facilitating easier processing in NLP tasks.\n",
    "   - Similarly, concatenates corresponding labels for each token within a sentence, preserving the sequence of tags for NER.\n",
    "\n",
    "4. **Data Deduplication**: \n",
    "   - Removes duplicate rows based on the `sentence` and `word_labels` columns, ensuring the uniqueness of data entries in the dataset.\n",
    "   - Resets the DataFrame index for easy data handling in subsequent operations.\n",
    "\n",
    "### Output\n",
    "\n",
    "- **Returns**: A `pandas` DataFrame containing two columns: `sentence` and `word_labels`. Each row represents a unique sentence and its associated sequence of labels, prepared for NER training.\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "- If an exception is encountered during the loading process, the function logs the error and re-raises the exception to halt further execution, ensuring issues are addressed promptly.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function is essential for transforming raw CSV data into a structured format suitable for NER tasks. By preprocessing and cleaning the data at this stage, the function facilitates smoother integration and efficiency in subsequent modeling steps, ensuring the data is ready for tokenization and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91c08b-1f6a-47a8-89ec-569bbffb1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, encoding='unicode_escape')\n",
    "        data = data.fillna(method='ffill')\n",
    "        data['sentence'] = data.groupby(['goldID'])['token'].transform(lambda x: ' '.join(x))\n",
    "        data['word_labels'] = data.groupby(['goldID'])['tag'].transform(lambda x: ','.join(x))\n",
    "        data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load data: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfba06f-2f6a-4c79-9bb8-f195d8b92858",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing Function\n",
    "\n",
    "The `load_data` function is crucial for the initial steps of loading and preprocessing the dataset required for training the token classification model. This function ensures the data is in an appropriate format for further processing and analysis.\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "- **Input**: Accepts the file path to a CSV file containing the dataset.\n",
    "- **Exception Handling**: Implements robust error handling to manage and log issues that occur during data loading.\n",
    "\n",
    "### Core Operations\n",
    "\n",
    "1. **CSV Data Loading**: \n",
    "   - Loads the dataset from the specified CSV file using `pandas`, with `unicode_escape` encoding to correctly handle special characters.\n",
    "   \n",
    "2. **Missing Value Handling**: \n",
    "   - Applies forward fill (`ffill`) to fill any missing values in the data. This method ensures that all data points have complete information by propagating the last valid observation forward.\n",
    "\n",
    "3. **Sentence and Label Aggregation**: \n",
    "   - Groups tokens by `goldID` and concatenates them to reconstruct full sentences, facilitating easier processing in NLP tasks.\n",
    "   - Similarly, concatenates corresponding labels for each token within a sentence, preserving the sequence of tags for NER.\n",
    "\n",
    "4. **Data Deduplication**: \n",
    "   - Removes duplicate rows based on the `sentence` and `word_labels` columns, ensuring the uniqueness of data entries in the dataset.\n",
    "   - Resets the DataFrame index for easy data handling in subsequent operations.\n",
    "\n",
    "### Output\n",
    "\n",
    "- **Returns**: A `pandas` DataFrame containing two columns: `sentence` and `word_labels`. Each row represents a unique sentence and its associated sequence of labels, prepared for NER training.\n",
    "\n",
    "### Error Handling\n",
    "\n",
    "- If an exception is encountered during the loading process, the function logs the error and re-raises the exception to halt further execution, ensuring issues are addressed promptly.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function is essential for transforming raw CSV data into a structured format suitable for NER tasks. By preprocessing and cleaning the data at this stage, the function facilitates smoother integration and efficiency in subsequent modeling steps, ensuring the data is ready for tokenization and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ffac38-bddf-4ddd-9b2b-f967476a34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "def prepare_datasets(data):\n",
    "    train_data = data.sample(frac=Config.TRAIN_SIZE, random_state=200)\n",
    "    test_data = data.drop(train_data.index).reset_index(drop=True)\n",
    "    return train_data.reset_index(drop=True), test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a1f10-9639-473b-8e12-758ec64faa3f",
   "metadata": {},
   "source": [
    "## Custom Dataset Class for NER Model\n",
    "\n",
    "The `SentenceDataset` class is an essential part of the token classification pipeline, designed to interface with PyTorch’s `DataLoader`. This custom class prepares and provides data in a format suitable for training and evaluating NER models based on transformers.\n",
    "\n",
    "### Class Definition\n",
    "\n",
    "- **Inheritance**: Extends PyTorch's `Dataset` class, ensuring compatibility with PyTorch workflows and functionalities such as batching and parallel data processing.\n",
    "\n",
    "### Constructor\n",
    "\n",
    "- **Parameters**:\n",
    "  - `dataframe`: A pandas DataFrame containing sentences and their corresponding labels.\n",
    "  - `tokenizer`: A tokenizer object from the transformers library, configured for a specific model.\n",
    "  - `max_len`: The maximum sequence length, ensuring uniformity in token length across data entries.\n",
    "\n",
    "### Methods\n",
    "\n",
    "1. **`__len__`**:\n",
    "   - Returns the number of entries in the dataset, allowing the `DataLoader` to determine the size of each batch and the number of iterations per epoch.\n",
    "\n",
    "2. **`__getitem__`**:\n",
    "   - Retrieves a single data point from the dataset by index.\n",
    "   - **Operations**:\n",
    "     - Extracts the sentence and its labels.\n",
    "     - Tokenizes the sentence while preserving label alignment with tokens, handling cases where a word is split into multiple subwords.\n",
    "     - Encodes the tokenized data into the format required by the model, including converting tokens to IDs, creating attention masks, and applying necessary padding.\n",
    "\n",
    "3. **`tokenize_and_preserve_labels`**:\n",
    "   - Tokenizes each word in the sentence and replicates each label to match the number of subwords generated from the corresponding word, ensuring that the label sequence remains aligned with the token sequence.\n",
    "\n",
    "4. **`encode_plus`**:\n",
    "   - Formats the tokenized input for the model by adding special tokens, converting tokens to input IDs, creating attention masks, and ensuring that all sequences are padded to a uniform length.\n",
    "   - **Padding Strategy**: Uses the model’s end-of-sequence token ID for padding if the standard padding token ID is not set, which is crucial for models like GPT-2.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "- Each call to `__getitem__` produces a dictionary with keys:\n",
    "  - `input_ids`: Token IDs suitable for model input.\n",
    "  - `attention_mask`: Mask specifying which tokens should be attended to, ignoring padding.\n",
    "  - `labels`: Numeric labels for each token, aligned with the `input_ids`.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This class is designed to streamline the preparation of data for NER models, transforming raw text data into structured inputs that are optimized for deep learning models. By handling complex preprocessing tasks such as tokenization, label alignment, and sequence padding, `SentenceDataset` ensures that the model can focus on learning from accurately prepared inputs.\n",
    "### Example Usage\n",
    "\n",
    "To integrate this class into a training or evaluation pipeline, instantiate it with the appropriate parameters and pass it to a PyTorch `DataLoader`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66603d0-ef7b-4f4a-8218-ea7bc0bf93ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.iloc[index]['sentence']\n",
    "        word_labels = self.data.iloc[index]['word_labels']\n",
    "        tokenized_sentence, labels = self.tokenize_and_preserve_labels(sentence, word_labels)\n",
    "        return self.encode_plus(tokenized_sentence, labels)\n",
    "\n",
    "    def tokenize_and_preserve_labels(self, sentence, text_labels):\n",
    "        tokenized_sentence, labels = [], []\n",
    "        for word, label in zip(sentence.split(), text_labels.split(',')):\n",
    "            subwords = self.tokenizer.tokenize(word)\n",
    "            tokenized_sentence.extend(subwords)\n",
    "            labels.extend([label] * len(subwords))\n",
    "        return tokenized_sentence, labels\n",
    "\n",
    "    def encode_plus(self, tokenized_sentence, labels):\n",
    "        tokenized_sentence = ['<|endoftext|>'] + tokenized_sentence[:self.max_len-2] + ['<|endoftext|>']\n",
    "        labels = ['O'] + labels[:self.max_len-2] + ['O']\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "        attention_mask = [1] * len(input_ids) + [0] * (self.max_len - len(input_ids))\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_len - len(input_ids)\n",
    "        input_ids += [self.tokenizer.eos_token_id] * padding_length  # Use eos_token_id for padding if pad_token_id is None\n",
    "        label_ids = [label2id.get(label, -100) for label in labels]\n",
    "        label_ids += [-100] * padding_length\n",
    "\n",
    "        return {'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "                'labels': torch.tensor(label_ids, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc640f34-244d-4f24-9485-55994673c240",
   "metadata": {},
   "source": [
    "## Training Function for NER Model\n",
    "\n",
    "The `train` function orchestrates the training process for a named entity recognition (NER) model, utilizing PyTorch's functionalities. This function is crucial for fitting the model to the training data, adjusting model weights through backpropagation based on the loss calculated from predictions.\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "- **Parameters**:\n",
    "  - `model`: The NER model to be trained.\n",
    "  - `loader`: A DataLoader that batches the dataset for efficient training.\n",
    "  - `optimizer`: The optimization algorithm used to update model weights.\n",
    "\n",
    "### Core Operations\n",
    "\n",
    "1. **Mode Setting**: \n",
    "   - Sets the model to training mode, which enables certain functionalities like dropout layers that are essential during training but not during evaluation.\n",
    "\n",
    "2. **Loss Initialization**:\n",
    "   - Initializes a counter for total loss to track the loss across all batches, providing a measure of training performance per epoch.\n",
    "\n",
    "3. **Batch Processing**:\n",
    "   - Iterates over each batch provided by the DataLoader. For each batch, performs the following steps:\n",
    "     - **Data Loading**: Moves input IDs, attention masks, and labels to the designated computing device (CPU or GPU).\n",
    "     - **Forward Pass**: Processes the inputs through the model, generating predictions and computing the loss.\n",
    "     - **Backward Pass**: Performs backpropagation to calculate gradients with respect to the loss.\n",
    "     - **Gradient Clipping**: Applies gradient norm clipping to prevent exploding gradients, a common issue in training deep neural networks.\n",
    "     - **Optimization Step**: Updates the model weights using the optimizer.\n",
    "\n",
    "4. **Logging**:\n",
    "   - Logs the loss for each batch to monitor training progress and diagnose training stability.\n",
    "\n",
    "### Output\n",
    "\n",
    "- **Returns**: The average loss for the epoch, computed by dividing the total loss by the number of batches processed. This metric helps in evaluating the training process across epochs.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function is designed to effectively train NER models by systematically adjusting their weights to minimize the loss on training data. The structured approach ensures that each step of the model training is carried out optimally, from data handling to weight updates, fostering robust learning.\n",
    "\n",
    "### Usage\n",
    "\n",
    "The `train` function is typically called within a training loop across multiple epochs, allowing the model to iteratively learn from the training data:\n",
    "\n",
    "```python\n",
    "for epoch in range(total_epochs):\n",
    "    epoch_loss = train(model, data_loader, optimizer)\n",
    "    print(f'Epoch {epoch}: Loss = {epoch_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2308b1-465f-443f-b04d-cd8b135e038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "def train(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        inputs, masks, labels = batch['input_ids'].to(Config.DEVICE), batch['attention_mask'].to(Config.DEVICE), batch['labels'].to(Config.DEVICE)\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids=inputs, attention_mask=masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), Config.MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        logging.info(f'Batch loss: {loss.item()}')  # Log loss for each batch\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbd7c9-d8ea-41fb-a0ab-98b617f1127b",
   "metadata": {},
   "source": [
    "## Evaluation Function for NER Model\n",
    "\n",
    "The `evaluate` function is designed to assess the performance of a trained named entity recognition (NER) model using a test or validation dataset. It calculates metrics such as precision, recall, and F1-score, which are essential for understanding the model's accuracy.\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "- **Parameters**:\n",
    "  - `model`: The NER model to be evaluated.\n",
    "  - `loader`: A DataLoader that batches the dataset for efficient evaluation.\n",
    "\n",
    "### Core Operations\n",
    "\n",
    "1. **Mode Setting**:\n",
    "   - Sets the model to evaluation mode, which disables training-specific operations like dropout to ensure consistent behavior for predictions.\n",
    "\n",
    "2. **Loss and Metric Initialization**:\n",
    "   - Initializes a counter for total loss and lists to store predictions and actual labels for later metric calculation.\n",
    "\n",
    "3. **Batch Processing**:\n",
    "   - Iterates over each batch from the DataLoader within a no-gradient context to save memory and prevent model updates.\n",
    "   - For each batch, performs the following steps:\n",
    "     - **Data Loading**: Transfers input IDs, attention masks, and labels to the designated compute device.\n",
    "     - **Forward Pass**: Computes the outputs and loss without updating model parameters.\n",
    "     - **Loss Accumulation**: Adds up the loss for each batch to calculate the total loss after all batches are processed.\n",
    "\n",
    "4. **Prediction Extraction and Label Mapping**:\n",
    "   - Extracts predictions using `argmax` on the logits to determine the most likely labels.\n",
    "   - Aligns predictions with their corresponding labels based on the attention mask, ensuring only non-padded elements are considered.\n",
    "\n",
    "5. **Metric Calculation**:\n",
    "   - Calculates precision, recall, and F1-score based on the gathered predictions and actual labels, providing a quantitative measure of the model’s performance.\n",
    "\n",
    "6. **Logging**:\n",
    "   - Logs calculated metrics for analysis and debugging purposes.\n",
    "\n",
    "### Output\n",
    "\n",
    "- **Returns**: A tuple containing the average loss per batch, precision, recall, and F1-score. These metrics help to quantify the model's effectiveness in identifying correct entities.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This function provides a comprehensive evaluation of the NER model, allowing developers and researchers to measure how well the model performs on unseen data. It highlights areas where the model excels and where it may need improvement, guiding further model tuning or deployment decisions.\n",
    "\n",
    "### Usage\n",
    "\n",
    "The `evaluate` function is typically called after the training process to assess the model's generalization capabilities:\n",
    "\n",
    "```python\n",
    "test_loss, precision, recall, f1 = evaluate(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5431b94-4f3f-4cd4-88ed-a83d10d74788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            inputs, masks, targets = batch['input_ids'].to(Config.DEVICE), batch['attention_mask'].to(Config.DEVICE), batch['labels'].to(Config.DEVICE)\n",
    "            outputs = model(input_ids=inputs, attention_mask=masks, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions_batch = torch.argmax(logits, axis=2)\n",
    "            for i, mask in enumerate(masks):\n",
    "                temp_1 = []\n",
    "                temp_2 = []\n",
    "                for j, m in enumerate(mask):\n",
    "                    if m and targets[i, j] != torch.tensor(-100):\n",
    "                        temp_1.append(id2label[targets[i, j].item()])\n",
    "                        temp_2.append(id2label[predictions_batch[i, j].item()])\n",
    "                labels.append(temp_1)\n",
    "                predictions.append(temp_2)\n",
    "\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "\n",
    "    logging.info(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
    "    return total_loss / len(loader), precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16864fa1-930e-4f67-8381-59c4ed599ce6",
   "metadata": {},
   "source": [
    "## Main Execution Flow for NER Model Training and Evaluation\n",
    "\n",
    "The `main` function serves as the entry point for the script, coordinating the data loading, model training, and evaluation phases. This function ensures that each component of the model training pipeline is executed in sequence and managed properly.\n",
    "\n",
    "### Process Overview\n",
    "\n",
    "1. **Data Loading**:\n",
    "   - Calls the `load_data` function to load and preprocess the dataset from a specified CSV file.\n",
    "\n",
    "2. **Dataset Preparation**:\n",
    "   - Uses the `prepare_datasets` function to split the loaded data into training and testing datasets.\n",
    "\n",
    "3. **Model Setup**:\n",
    "   - Iterates over predefined model configurations stored in `model_names`.\n",
    "   - Initializes the tokenizer and model for each specified GPT-2 variant.\n",
    "   - Configures the model to run on the appropriate device (CPU or GPU).\n",
    "\n",
    "4. **DataLoader Configuration**:\n",
    "   - Sets up PyTorch `DataLoader`s for both training and testing datasets, specifying batch sizes and shuffle settings.\n",
    "\n",
    "5. **Optimizer Initialization**:\n",
    "   - Configures the Adam optimizer with a predefined learning rate, ready to update model weights during training.\n",
    "\n",
    "6. **Training Loop**:\n",
    "   - Executes a training loop over a set number of epochs, using the `train` function to process the training data.\n",
    "   - Logs the training loss after each epoch to monitor progress.\n",
    "\n",
    "7. **Final Evaluation**:\n",
    "   - In the last epoch, evaluates the model on the test dataset using the `evaluate` function.\n",
    "   - Captures and logs key performance metrics: precision, recall, and F1 score.\n",
    "\n",
    "8. **Results Handling**:\n",
    "   - Prints and logs the final test metrics for quick reference and analysis.\n",
    "   - Saves the evaluation metrics to a CSV file in a specified results directory for further analysis or reporting.\n",
    "\n",
    "### Purpose\n",
    "\n",
    "This main function is critical for executing the full lifecycle of a machine learning project—from data handling through to model training and performance evaluation. It encapsulates all necessary steps in a clear and logical sequence, ensuring that the model is trained and evaluated effectively.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "To run this script, ensure it is executed as the main module:\n",
    "\n",
    "```bash\n",
    "python script_name.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ef93e1-4438-4be5-ad3c-2cf69e693b13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'Test Loss': 0.01575318283615464, 'Precision': 0.8421052631578947, 'Recall': 0.7529411764705882, 'F1 Score': 0.7950310559006211}\n",
      "*********************************************************************************************\n",
      "Saved test metrics to ./results_Anxiety_NER_BERT&GPT/gpt2_test_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "def main():\n",
    "    data = load_data(Config.DATA_FILE)\n",
    "    train_data, test_data = prepare_datasets(data)\n",
    "    for model_key in model_names:\n",
    "        logging.info(f\"Training and evaluating model: {model_names[model_key]}\")\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_key)\n",
    "        model = GPT2ForTokenClassification.from_pretrained(model_key, num_labels=len(id2label))\n",
    "        model.to(Config.DEVICE)\n",
    "        train_loader = DataLoader(SentenceDataset(train_data, tokenizer, Config.MAX_LEN), batch_size=Config.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(SentenceDataset(test_data, tokenizer, Config.MAX_LEN), batch_size=Config.VALID_BATCH_SIZE, shuffle=False)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "            train_loss = train(model, train_loader, optimizer)\n",
    "            logging.info(f'Epoch {epoch+1}, Train Loss: {train_loss}')\n",
    "            if epoch == Config.EPOCHS - 1:  # Only evaluate and save in the last epoch\n",
    "                test_loss, precision, recall, f1 = evaluate(model, test_loader)\n",
    "                test_metrics = {'Test Loss': test_loss, 'Precision': precision, 'Recall': recall, 'F1 Score': f1}\n",
    "\n",
    "\n",
    "         # Now outside the loop, only for the final epoch\n",
    "    print(f\"Test metrics: {test_metrics}\")\n",
    "    print(\"*********************************************************************************************\")\n",
    "        \n",
    "    # Save metrics\n",
    "    metrics_filename = os.path.join(Config.RESULTS_DIR, f\"{model_names[model_key]}_test_metrics.csv\")\n",
    "    pd.DataFrame([test_metrics]).to_csv(metrics_filename, index=False)\n",
    "    print(f\"Saved test metrics to {metrics_filename}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad81e6-e0cd-41ec-9b7b-e36508a278a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0595416f-463d-43f8-b98c-d14213e2fd14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
